{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uDm_84sHAYs",
        "outputId": "cc77ec8b-d892-458e-9210-37c51e6ac5c2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import regex as re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "from collections import defaultdict\n",
        "import math\n",
        "nltk.download('stopwords')\n",
        "\n",
        "sentences = list()\n",
        "sentence = list()\n",
        "with open('/content/annotations (5).tsv') as file:\n",
        "  for line in file:\n",
        "      if line != \"\\n\":\n",
        "        sentence.append(line.split('\\t')[1])\n",
        "      if line == \"\\n\" and sentence:\n",
        "        sentences.append(sentence)\n",
        "        sentence = list()\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "clear_data = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZyT1Cu8MRB5",
        "outputId": "60d54b9e-06b0-43ec-c8ed-e75f573fdfc4"
      },
      "outputs": [],
      "source": [
        "def clean_sentence(sentence):\n",
        "    cleaned = []\n",
        "    for lemma in sentence:\n",
        "        cleaned_tek = re.sub(r\"[^\\w\\s]|[\\d]\", \"\", lemma.lower())\n",
        "        if len(cleaned_tek) != len(lemma) or len(cleaned_tek) <3:\n",
        "          continue;\n",
        "\n",
        "        if cleaned_tek and cleaned_tek not in stop_words:\n",
        "            cleaned.append(cleaned_tek)\n",
        "    return cleaned\n",
        "\n",
        "clear_data = []  # Инициализация списка для хранения очищенных данных\n",
        "\n",
        "for sentence in sentences:\n",
        "    clear_data.append(clean_sentence(sentence))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlN9B6FnHgD3"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uwf_6gtIdWM5"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "w2v = Word2Vec(sentences=clear_data, epochs=70, window=4, min_count=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujTMwCGvi-uP",
        "outputId": "35575f1f-1123-4e60-a47f-bd907192fad5"
      },
      "outputs": [],
      "source": [
        "w2v.wv.most_similar(\"subject\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37bUeupgiRC4",
        "outputId": "7f262ded-aedb-4c4a-bd75-6b2b46d17f93"
      },
      "outputs": [],
      "source": [
        "# Получение всех слов\n",
        "words = list(w2v.wv.key_to_index.keys())\n",
        "print(\"Все слова в модели:\", words)\n",
        "\n",
        "word_vector = w2v.wv['win'] \n",
        "print(\"Вектор для слова 'win':\", word_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzvwNkz3ZCdq",
        "outputId": "1311d414-d7e5-4752-ddb4-e3c83e9207e0"
      },
      "outputs": [],
      "source": [
        "len(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7iT9-c_A3Gu",
        "outputId": "fdfb922c-d926-41e1-f9df-89b605b9e580"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import seaborn as sns\n",
        "\n",
        "def cosine_distance(a, b):\n",
        "    return (1 - np.dot(a, b)/(np.linalg.norm(a) * np.linalg.norm(b))) / 2\n",
        "\n",
        "word = \"computer\"\n",
        "similar_words = [\"software\", \"machine\", \"hardware\"]  # Семантически близкие\n",
        "related_words = [\"program\", \"cpu\", \"system\"]   # Слова из той же предметной области\n",
        "distant_words = [\"god\", \"religion\", \"church\"]  # Семантически далекие\n",
        "\n",
        "for similar_word in similar_words:\n",
        "    print(f\"Косинусное расстояние между {word} и {similar_word}: {cosine_distance(w2v.wv[word], w2v.wv[similar_word])}\")\n",
        "\n",
        "for related_word in related_words:\n",
        "    print(f\"Косинусное расстояние между {word} и {related_word}: {cosine_distance(w2v.wv[word], w2v.wv[related_word])}\")\n",
        "\n",
        "for distant_word in distant_words:\n",
        "    print(f\"Косинусное расстояние между {word} и {distant_word}: {cosine_distance(w2v.wv[word], w2v.wv[distant_word])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "AVxQBoXMjyxS",
        "outputId": "9331e08a-1618-4817-cc65-b63b11a570c2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_word_vectors(words, vector_data):\n",
        "    #получение двумерного представления\n",
        "    pca_model = PCA(n_components=2)\n",
        "    reduced_vectors = pd.DataFrame(pca_model.fit_transform([vector_data[word] for word in words]))\n",
        "\n",
        "    reduced_vectors.index = words\n",
        "    reduced_vectors.columns = [\"x_coord\", \"y_coord\"]\n",
        "\n",
        "    scatter_plot = sns.scatterplot(data=reduced_vectors, x=\"x_coord\", y=\"y_coord\")\n",
        "\n",
        "    # Добавление меток для каждой точки\n",
        "    for word in reduced_vectors.index:\n",
        "        coordinates = reduced_vectors.loc[word]\n",
        "        scatter_plot.text(coordinates.x_coord, coordinates.y_coord, word)\n",
        "\n",
        "    return scatter_plot\n",
        "\n",
        "words_to_plot = [\"car\", \"bike\", \"jesus\", \"road\", \"religion\", \"computer\"]\n",
        "plot_word_vectors(words_to_plot, w2v.wv)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ym70N2Moc-0d"
      },
      "source": [
        "Реализовать метод, осуществляющий векторизацию произвольного текста по следующему алгоритму"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fbKZp52uEaM",
        "outputId": "fd503f04-5baa-49e2-bb56-32024f6c42fd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def calculate_vectors(sentences, w2v):\n",
        "    final_vector = np.zeros(w2v.vector_size)\n",
        "\n",
        "    for sentence in sentences:\n",
        "        current_sentence_vector = np.zeros(w2v.vector_size)\n",
        "\n",
        "        for word in sentence:\n",
        "            if word in w2v.wv.key_to_index:\n",
        "                current_sentence_vector += w2v.wv[word]\n",
        "\n",
        "        if len(sentence) > 0:\n",
        "            current_sentence_vector /= len(sentence)\n",
        "\n",
        "        final_vector += current_sentence_vector\n",
        "\n",
        "    if len(sentences) > 0:\n",
        "        final_vector /= len(sentences)\n",
        "\n",
        "    return final_vector\n",
        "\n",
        "calculate_vectors(clear_data, w2v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GT31f7S8SrmO",
        "outputId": "25ea793f-95a5-4b77-ce1f-b3baa03eeb9b"
      },
      "outputs": [],
      "source": [
        "!unzip /content/annotated-corpus.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZuetI63iSTUX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import chardet\n",
        "from scipy.sparse import csr_matrix, save_npz\n",
        "def detect_encoding(file_path):\n",
        "\n",
        "    with open(file_path, 'rb') as f:\n",
        "        result = chardet.detect(f.read(10000))\n",
        "    return result['encoding']\n",
        "def process_file(file_path):\n",
        "\n",
        "    sentences = []\n",
        "    sentence = []\n",
        "\n",
        "    with open(file_path, 'r', encoding=detect_encoding(file_path)) as file:\n",
        "        for line in file:\n",
        "            if line != \"\\n\":\n",
        "                sentence.append(line.split('\\t')[1])\n",
        "            if line == \"\\n\" and sentence:\n",
        "                sentences.append(sentence)\n",
        "                sentence = []\n",
        "\n",
        "    clear_data = []\n",
        "    for sent in sentences:\n",
        "        clear_data.append(clean_sentence(sent))\n",
        "\n",
        "    return clear_data\n",
        "\n",
        "def save_vectors_to_tsv(file_vectors, output_tsv_path):\n",
        "\n",
        "    with open(output_tsv_path, 'w', encoding='utf-8') as f:\n",
        "        for doc_id, vector in file_vectors.items():\n",
        "            vector_str = '\\t'.join(map(str, vector))\n",
        "            f.write(f\"{doc_id}\\t{vector_str}\\n\")\n",
        "\n",
        "\n",
        "def process_directory(directory_path, output_tsv_path):\n",
        "\n",
        "    file_vectors = {}\n",
        "\n",
        "    for filename in os.listdir(directory_path):\n",
        "        if filename.endswith(\".tsv\"):\n",
        "            file_path = os.path.join(directory_path, filename)\n",
        "            doc_id = os.path.splitext(os.path.basename(file_path))[0]\n",
        "            clear_data = process_file(file_path)\n",
        "            document_vector = calculate_vectors(clear_data, w2v)\n",
        "            file_vectors[doc_id] = document_vector\n",
        "\n",
        "    save_vectors_to_tsv(file_vectors, output_tsv_path)\n",
        "    return file_vectors\n",
        "\n",
        "\n",
        "directory = '/content/space'\n",
        "\n",
        "\n",
        "doc_vec = process_directory(directory, 'space_vec.tsv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKMAvge7QeYH"
      },
      "outputs": [],
      "source": [
        "flat_list = [item for sublist in clear_data for item in sublist]\n",
        "unique_words = list(set(flat_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MT5BXMCDQroC"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2T99B7-Z1G-"
      },
      "source": [
        "1 задание - сохранение в файл частот токенов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUwT7p5JTiOu"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import os\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "def build_token_dictionary_frec(tokens):\n",
        "  for token in tokens:\n",
        "      token_freqs[token] += 1\n",
        "  return token_freqs\n",
        "\n",
        "\n",
        "token_freqs = build_token_dictionary_frec(flat_list)\n",
        "\n",
        "\n",
        "def filter_low_frequency_tokens(token_freqs, min_freq=5):\n",
        "    return {token: freq for token, freq in token_freqs.items() if freq >= min_freq}\n",
        "\n",
        "filtered_token_freqs = filter_low_frequency_tokens(token_freqs)\n",
        "\n",
        "\n",
        "def save_token_frequencies(token_freqs, output_file):\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for token, freq in token_freqs.items():\n",
        "            f.write(f\"{token}\\t{freq}\\n\")\n",
        "\n",
        "save_token_frequencies(filtered_token_freqs, 'token_frequencies.tsv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ijwX-GTZ6nV"
      },
      "source": [
        "1 задание - сохранение матрицы термин документ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gq8nnxPSTUHG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "def process_directory(directory_path):\n",
        "    doc_texts = {}\n",
        "    for filename in os.listdir(directory_path):\n",
        "        if filename.endswith(\".tsv\"):\n",
        "            file_path = os.path.join(directory_path, filename)\n",
        "            doc_id = os.path.splitext(filename)[0]  # Удаляем расширение\n",
        "            doc_texts[doc_id] = [item for sublist in process_file(file_path) for item in sublist]\n",
        "    return doc_texts\n",
        "\n",
        "def create_term_doc_matrix(doc_texts):\n",
        "    # Собираем все уникальные токены\n",
        "    token_freq = defaultdict(int)\n",
        "    for doc_id, tokens in doc_texts.items():\n",
        "        for token in tokens:\n",
        "            token_freq[token] += 1\n",
        "    unique_tokens = list(token_freq.keys())\n",
        "\n",
        "    # Маппинг токенов в индексы\n",
        "    token_index = {token: i for i, token in enumerate(unique_tokens)}\n",
        "\n",
        "    # Создаем разреженную матрицу для хранения данных\n",
        "    rows = []\n",
        "    cols = []\n",
        "    data = []\n",
        "\n",
        "    doc_index = {doc_id: idx for idx, doc_id in enumerate(doc_texts.keys())}  # Маппинг документов в индексы\n",
        "\n",
        "    for doc_id, tokens in doc_texts.items():\n",
        "        doc_idx = doc_index[doc_id]\n",
        "        token_counts = Counter(tokens)\n",
        "        for token, count in token_counts.items():\n",
        "            if token in token_index:\n",
        "                token_idx = token_index[token]\n",
        "                rows.append(doc_idx)\n",
        "                cols.append(token_idx)\n",
        "                data.append(count)\n",
        "\n",
        "    term_doc_matrix = csr_matrix((data, (rows, cols)), shape=(len(doc_texts), len(unique_tokens)))\n",
        "\n",
        "    return term_doc_matrix, doc_index, token_index, token_freq\n",
        "\n",
        "def save_term_doc_matrix_to_tsv(term_doc_matrix, doc_index, token_index, output_tsv_path, unique_tokens):\n",
        "    dense_matrix = term_doc_matrix.toarray()\n",
        "    with open(output_tsv_path, 'w', encoding='utf-8') as tsv_file:\n",
        "        # Записываем заголовок\n",
        "        header = ['doc_id'] + unique_tokens\n",
        "        tsv_file.write('\\t'.join(header) + '\\n')\n",
        "\n",
        "        # Записываем строки\n",
        "        for doc_id, doc_idx in doc_index.items():\n",
        "            row = [doc_id] + list(map(str, dense_matrix[doc_idx]))\n",
        "            tsv_file.write('\\t'.join(row) + '\\n')\n",
        "\n",
        "        # Вычисляем сумму для каждого токена и записываем в конец\n",
        "        token_sums = dense_matrix.sum(axis=0)  # A1 преобразует разреженный массив в обычный одномерный\n",
        "        sum_row = ['Total'] + list(map(str, token_sums))\n",
        "        tsv_file.write('\\t'.join(sum_row) + '\\n')\n",
        "\n",
        "directory_path = '/content/electronics'\n",
        "output_tsv_path = 'term_doc_matrix.tsv'\n",
        "\n",
        "doc_texts = process_directory(directory_path)\n",
        "\n",
        "term_doc_matrix, doc_index, token_index, token_freq = create_term_doc_matrix(doc_texts)\n",
        "unique_tokens = list(token_freq.keys())\n",
        "save_term_doc_matrix_to_tsv(term_doc_matrix, doc_index, token_index, output_tsv_path, unique_tokens)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
