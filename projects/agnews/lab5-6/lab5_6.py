# -*- coding: utf-8 -*-
"""lab5-6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SAbEvXY5iZrAHR_zbnr-7GXPze2nAk0j
"""

!pip install openai
!pip install evaluate
!pip install llama-cpp-python
!pip install pinecone-client
!pip install langchain==0.0.300
!pip install --upgrade chromadb
!pip install sentence-transformers==2.2.2
!pip uninstall -y sentence-transformers huggingface_hub
!pip install "sentence-transformers>=2.2.2" "huggingface_hub<1.0.0"

from langchain.document_loaders import PDFMinerLoader, TextLoader, CSVLoader, UnstructuredWordDocumentLoader, UnstructuredHTMLLoader
from langchain.embeddings import HuggingFaceEmbeddings, OpenAIEmbeddings
from langchain.text_splitter import SentenceTransformersTokenTextSplitter
from multiprocessing.pool import ThreadPool
from langchain.vectorstores import Chroma
from langchain.schema import Document as LCDocument
from chromadb.config import Settings
from typing import Any
from tqdm import tqdm

import uuid
from dataclasses import dataclass
from typing import List, Dict

import chromadb
from sentence_transformers import SentenceTransformer
import pandas as pd
import statistics
import time
import tqdm
import glob
import os

"""##Loader"""

from google.colab import drive
drive.mount('/content/drive')

from dataclasses import dataclass
from typing import List
import pyarrow.parquet as pq
import hashlib

@dataclass
class Document:
    content: str
    metadata: dict

import uuid
import pandas as pd
import pyarrow.parquet as pq
from typing import List

class DocumentLoader:
    def load_documents(self, parquet_file_path: str) -> List[Document]:
        """
        Загрузка документов из Parquet-файла.

        Читает Parquet-файл, извлекает содержимое и метаданные,
        создавая список объектов Document.
        """
        # Чтение Parquet-файла в DataFrame
        document_data = pd.read_parquet(parquet_file_path)

        # Список для хранения документов
        document_list = []

        # Итерация по строкам DataFrame и создание объектов Document
        for _, row in document_data.iterrows():
            document = Document(
                content=row['text'],
                metadata={
                    'category': row['label'],
                    'unique_id': str(uuid.uuid4())  # Генерация уникального ID
                }
            )
            document_list.append(document)

        return document_list

    def load_documents_in_chunks(self, parquet_file_path: str, batch_size: int = 1000) -> List[Document]:
        """
        Постепенная загрузка документов из большого Parquet-файла.

        Читает Parquet-файл по частям (батчами) и создает объекты Document.
        """
        parquet_file = pq.ParquetFile(parquet_file_path)
        document_list = []

        # Итерация по батчам и обработка каждой строки
        for batch in parquet_file.iter_batches(batch_size=batch_size):
            document_data = batch.to_pandas()

            for _, row in document_data.iterrows():
                document = Document(
                    content=row['text'],
                    metadata={
                        'category': row['label'],
                        'unique_id': str(uuid.uuid4())  # Генерация уникального ID
                    }
                )
                document_list.append(document)

        return document_list

"""##Splitter"""

from sentence_transformers import SentenceTransformer
from typing import List

class DocumentSplitter:
    def __init__(
        self,
        chunk_size: int = 128,
        chunk_overlap: int = 32,
        model_name: str = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
    ):
        self.splitter = SentenceTransformersTokenTextSplitter(
            model_name=model_name,
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            add_start_index=True
        )
        self.tokenizer = SentenceTransformer(model_name).tokenizer

    def split_documents(self, input_documents: List[Document]) -> List[Document]:
        document_chunks = []
        for document in input_documents:
            text_segments = self.splitter.split_text(document.content)
            tokenized_segments = self.tokenizer(text_segments, padding=False, truncation=False)["input_ids"]

            for segment_text, segment_tokens in zip(text_segments, tokenized_segments):
                updated_metadata = document.metadata.copy()
                updated_metadata.update({
                    "segment_tokens": len(segment_tokens),
                    "original_length": len(document.content),
                    "segment_hash": hash(segment_text)
                })
                document_chunks.append(Document(content=segment_text, metadata=updated_metadata))

        return document_chunks

"""##Vector database"""

from typing import List, Dict
from sentence_transformers import SentenceTransformer

class DocumentCollector:
    def add_documents(self, texts: List[str], metadata: List[Dict]) -> None:
        """
        Добавляет текстовые данные и метаданные в коллекцию.
        """
        raise NotImplementedError

    def add_documents_from_directory(self, directory_path: str) -> None:
        """
        Добавляет документы из указанной директории в коллекцию.
        """
        raise NotImplementedError

    def search_documents(self, query_strings: List[str], top_n: int) -> List[Document]:
        """
        Выполняет поиск документов по запросам и возвращает топ-n результатов.
        """
        raise NotImplementedError

    def get_documents_by_query(self, query_string: str, top_n: int, score_threshold: float) -> List[Document]:
        """
        Выполняет поиск документов по запросу с порогом для оценки релевантности.
        """
        raise NotImplementedError

    def clear_collection(self) -> None:
        """
        Очищает коллекцию документов.
        """
        raise NotImplementedError


class TextEmbedder:
    def get_embedder(self):
        """
        Возвращает объект, осуществляющий эмбеддинг текста.
        """
        raise NotImplementedError


class HuggingFaceTextEmbedder(TextEmbedder):
    def __init__(self, model_name: str = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"):
        self.model = SentenceTransformer(model_name)
        self.model_name = model_name

    def get_embedder(self):
        """
        Возвращает объект SentenceTransformer для эмбеддинга.
        """
        return self.model

    def encode_texts(self, texts: List[str]) -> List[List[float]]:
        """
        Преобразует список текстов в эмбеддинги.
        """
        return self.model.encode(
            texts,
            convert_to_tensor=False,
            normalize_embeddings=True
        ).tolist()

from tqdm import tqdm
import os
import shutil
from google.colab import drive

class ChromaDocumentCollector(DocumentCollector):
    def __init__(
        self,
        embedder: HuggingFaceTextEmbedder,
        loader: DocumentLoader = None,
        splitter: DocumentSplitter = None,
        collection_name: str = "documents",
        persist_dir: str = "chroma_db",
        chunk_size: int = 128,
        chunk_overlap: int = 32,
        max_retries: int = 3,
        batch_size: int = 1000,
    ):
        self.embedder = embedder
        self.loader = loader or DocumentLoader()
        self.splitter = splitter or DocumentSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            model_name=embedder.model_name
        )
        self.persist_dir = persist_dir
        self.collection_name = collection_name
        self.max_retries = max_retries
        self.batch_size = batch_size
        self.distance_metric = "cosine"

        self._init_chroma_client()

    def _init_chroma_client(self):
        self.client = chromadb.PersistentClient(path=self.persist_dir)
        self.collection = self.client.get_or_create_collection(
            name=self.collection_name,
            metadata={"hnsw:space": self.distance_metric}
        )

    def save_index_to_gdrive(self, gdrive_path: str):
        """Сохраняет индекс на Google Drive"""
        try:
            drive.mount('/content/drive')
            source_dir = self.persist_dir
            target_dir = f"/content/drive/MyDrive/{gdrive_path}"

            # Копируем с перезаписью
            if os.path.exists(target_dir):
                shutil.rmtree(target_dir)
            shutil.copytree(source_dir, target_dir)

            print(f"Индекс сохранен в Google Drive: {target_dir}")
        except Exception as e:
            raise RuntimeError(f"Ошибка сохранения: {str(e)}")

    def load_index_from_gdrive(self, gdrive_path: str):
        """Загружает индекс с Google Drive"""
        try:
            drive.mount('/content/drive')
            source_dir = f"/content/drive/MyDrive/{gdrive_path}"
            target_dir = self.persist_dir

            if not os.path.exists(source_dir):
                raise FileNotFoundError("Индекс не найден на Google Drive")

            # Очищаем локальную копию
            if os.path.exists(target_dir):
                shutil.rmtree(target_dir)

            # Копируем данные
            shutil.copytree(source_dir, target_dir)

            # Переинициализируем клиента
            self._init_chroma_client()
            print(f"Индекс загружен из Google Drive: {source_dir}")
        except Exception as e:
            raise RuntimeError(f"Ошибка загрузки: {str(e)}")

    def _process_documents(self, texts: List[str], metadata: List[Dict]) -> tuple:
        temp_documents = [
            Document(content=text, metadata=meta)
            for text, meta in zip(texts, metadata)
        ]

        chunks = self.splitter.split_documents(temp_documents)

        chunk_texts = [chunk.content for chunk in chunks]
        chunk_metadata = [chunk.metadata for chunk in chunks]

        return chunk_texts, chunk_metadata

    def add_documents(self, texts: List[str], metadata: List[Dict]):
        chunk_texts, chunk_metadata = self._process_documents(texts, metadata)

        # Рассчитываем общее количество батчей
        total_batches = (len(chunk_texts) + self.batch_size - 1) // self.batch_size

        # Создаем прогресс-бар
        with tqdm(total=total_batches, desc="Добавление батчей", unit="batch") as pbar:
            for i in range(0, len(chunk_texts), self.batch_size):
                batch_chunks = chunk_texts[i:i+self.batch_size]
                batch_metadata = chunk_metadata[i:i+self.batch_size]
                success = False

                for attempt in range(self.max_retries):
                    try:
                        # Кодирование и добавление
                        embeddings = self.embedder.encode(batch_chunks)
                        ids = [str(uuid.uuid4()) for _ in batch_chunks]

                        self.collection.add(
                            ids=ids,
                            embeddings=embeddings,
                            metadatas=batch_metadata,
                            documents=batch_chunks
                        )
                        success = True
                        break
                    except Exception as e:
                        pbar.write(f"Ошибка в батче {i//self.batch_size}: {str(e)}")
                        if attempt < self.max_retries - 1:
                            sleep_time = 2 ** attempt
                            pbar.write(f"Повтор через {sleep_time} сек...")
                            time.sleep(sleep_time)

                if success:
                    pbar.update(1)
                    pbar.set_postfix({
                        "добавлено": f"{min(i+self.batch_size, len(chunk_texts))}/{len(chunk_texts)}"
                    })
                else:
                    raise RuntimeError(f"Не удалось добавить батч {i//self.batch_size} после {self.max_retries} попыток")

    def add_documents_from_directory(self, directory_path: str):
        documents = self.loader.load_documents(directory_path)
        texts = [doc.content for doc in documents]
        metadata = [doc.metadata for doc in documents]
        self.add_documents(texts, metadata)

    def search_documents(self, query_strings: List[str], top_n: int) -> List[Document]:
        embeddings = self.embedder.encode(query_strings)
        results = self.collection.query(
            query_embeddings=embeddings,
            n_results=top_n,
            include=["documents", "metadatas"]
        )
        return self._format_results(results)

    def get_documents_by_query(self, query_string: str, top_n: int, score_threshold: float) -> List[Document]:
        embedding = self.embedder.encode([query_string])[0]

        results = self.collection.query(
            query_embeddings=[embedding],
            n_results=top_n,
            include=["documents", "metadatas", "distances"]
        )

        # Фильтрация результатов по порогу
        filtered_documents = []
        for doc, meta, distance in zip(results["documents"][0],
                                        results["metadatas"][0],
                                        results["distances"][0]):
            # Конвертируем расстояние в схожесть для косинусной метрики
            if self.distance_metric == "cosine":
                similarity = 1 - distance
            else:
                similarity = distance

            if similarity >= score_threshold:
                new_metadata = meta.copy()
                new_metadata["similarity"] = similarity
                filtered_documents.append(Document(content=doc, metadata=new_metadata))

        # Сортировка по убыванию схожести
        filtered_documents.sort(key=lambda x: x.metadata["similarity"], reverse=True)

        return filtered_documents[:top_n]

    def clear_collection(self):
        self.client.reset()

    def _format_results(self, results) -> List[Document]:
        formatted_documents = []
        for doc, meta in zip(results["documents"][0], results["metadatas"][0]):
            formatted_documents.append(Document(
                content=doc,
                metadata=meta
            ))
        return formatted_documents

"""###Implementation vector database"""

document_loader = DocumentLoader()
text_embedder = HuggingFaceTextEmbedder()
document_collector = ChromaDocumentCollector(
    embedder=text_embedder,
    loader=document_loader,
    persist_dir="documents",
    chunk_size=128
)

document_collector.add_documents_from_directory("/train.parquet")

document_collector.save_index_to_gdrive("/indexes")



"""##Search"""

n_results = 3 #@param {type:"integer"}
score_threshold = 0.5 # @param {type:"slider", min:0, max:1, step:0.1}

collector.get_documents("Brexit negotiation", n_results, 0.6)

collector.get_documents("Bitcoin", n_results, 0.5)

"""# Lab 6"""

import torch
from transformers import pipeline
from typing import List

class QASystem:
    def __init__(self, chroma_collector):
        # Инициализация Chroma
        self.chroma = chroma_collector

        # Инициализация пайплайна
        self.pipe = pipeline(
            "text-generation",
            model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            torch_dtype=torch.bfloat16,
            device_map="auto"
        )

    def generate_answer(self, question: str, top_n: int = 2) -> str:
        # Получение контекста
        context_docs = self.chroma.get_documents(question, n_results=top_n, score_threshold=0.5)
        context = "\n".join([doc.content for doc in context_docs]) if context_docs else "No context available"

        # Формирование сообщений
        messages = [
            {
                "role": "system",
                "content": f"Answer the question using only this context: {context}"
            },
            {
                "role": "user",
                "content": question
            }
        ]

        # Создание промпта
        prompt = self.pipe.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        # Генерация ответа
        outputs = self.pipe(
            prompt,
            max_new_tokens=256,
            do_sample=True,
            temperature=0.7,
            top_k=50,
            top_p=0.95
        )

        # Извлечение ответа
        full_response = outputs[0]["generated_text"]
        return full_response.split("<|assistant|>")[-1].strip()

general_qa_pairs = [
    (
        "What is photosynthesis?",
        "Photosynthesis is the process by which plants convert sunlight, water, and carbon dioxide into glucose and oxygen using chlorophyll."
    ),
    (
        "How does a blockchain work?",
        "Blockchain is a decentralized digital ledger that records transactions across multiple computers in encrypted blocks linked in a chronological chain."
    ),
    (
        "What causes earthquakes?",
        "Earthquakes occur due to sudden energy release from tectonic plate movements along fault lines in Earth's crust."
    ),
    (
        "What are the main components of blood?",
        "Blood consists of plasma (55%), red blood cells (erythrocytes), white blood cells (leukocytes), and platelets (thrombocytes)."
    ),
    (
        "How do vaccines work?",
        "Vaccines stimulate the immune system by introducing weakened or inactivated pathogens to create antibody memory."
    ),
    (
        "What is the greenhouse effect?",
        "The greenhouse effect is the trapping of heat in Earth's atmosphere by gases like CO2 and methane, maintaining Earth's temperature."
    ),
    (
        "Explain Newton's laws of motion",
        "1) Inertia: Objects maintain motion unless acted on; 2) F=ma; 3) Every action has equal and opposite reaction."
    ),
    (
        "What is machine learning?",
        "Machine learning is a subset of AI where algorithms improve automatically through experience and data patterns."
    ),
    (
        "How do airplanes stay airborne?",
        "Airplanes achieve lift through wing shape (airfoil) creating pressure difference between upper and lower surfaces."
    ),
    (
        "What causes ocean tides?",
        "Tides are primarily caused by gravitational interactions between Earth, Moon, and Sun, with lunar gravity being dominant."
    )
]

qa = QASystem(collector)

qa.generate_answer(general_qa_pairs[0][0])

!pip install bert_score

from bert_score import score

def calculate_bertscore(results):
    references = [res['true_answer'] for res in results]
    candidates = [res['predicted_answer'] for res in results]

    P, R, F1 = score(candidates, references, lang="en", verbose=True)

    for i, res in enumerate(results):
        res['bertscore'] = {
            "precision": P[i].item(),
            "recall": R[i].item(),
            "f1": F1[i].item()
        }

    return results

def evaluate(qa_system, test_pairs):
    results = []
    for question, true_answer in test_pairs:
        answer = qa_system.generate_answer(question)
        results.append({
            "question": question,
            "true_answer": true_answer,
            "predicted_answer": answer,
            "bertscore": None
        })
        results = calculate_bertscore(results)
    return results

# Запуск оценки
evaluation_results = evaluate(qa, general_qa_pairs)

# Тестирование
for result in evaluation_results:
    print(f"question: {result['question']}")
    print(f"Expected: {result['true_answer']}")
    print(f"Actual: {result['predicted_answer']}")
    print("Bert score metrics:")
    print(result['bertscore'])
    print('-'*50)

