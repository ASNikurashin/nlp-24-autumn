# -*- coding: utf-8 -*-
"""lab3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UDZ8aF4_24j8Wv5gAZBk4kQNPmvl0qaM
"""

import os
import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm
from gensim.models import Word2Vec
import nltk
from nltk.corpus import stopwords
from typing import Union, List

nltk.download('stopwords')

# Функция очистки токенов
def prepare_data(tokens):
    pattern = r'[^\w\s]'
    stop_words = set(stopwords.words('english'))
    return [re.sub(pattern, '', str(token).lower()) for token in tokens
            if re.sub(pattern, '', str(token).lower()) not in stop_words and token]

# Чтение предложений из TSV-файла
def read_tsv_sentences(file_path: str) -> list[list[str]]:
    df = pd.read_csv(file_path, sep='\t', usecols=[0], header=None, names=['word'],
                     skip_blank_lines=False, na_filter=False)
    words = df['word'].astype(str).str.strip().tolist()
    sentences, current_sentence = [], []

    for word in words:
        if not word:
            if current_sentence:
                sentences.append(current_sentence)
                current_sentence = []
        else:
            current_sentence.append(word)

    if current_sentence:
        sentences.append(current_sentence)

    return sentences

# Обработка данных
train_data, test_data = [], []
base_path = '../assets/annotated-corpus/'

for data_type in tqdm(['train', 'test'], desc='Datasets'):
    data_path = os.path.join(base_path, data_type)
    if not os.path.exists(data_path):
        continue

    for class_name in tqdm(['0', '1', '2', '3'], desc=f'Classes ({data_type})'):
        class_path = os.path.join(data_path, class_name)
        docs = [f for f in os.listdir(class_path) if f.endswith('.tsv') and os.path.isfile(os.path.join(class_path, f))]

        for doc_name in docs:
            sentences = read_tsv_sentences(os.path.join(class_path, doc_name))
            entry = {"doc_id": os.path.splitext(doc_name)[0], "class": class_name, "words": sentences}
            (train_data if data_type == 'train' else test_data).append(entry)

model_path = "word2vec_v2.model"

def train_word2vec(train_data, model_path='word2vec.model'):
    model = Word2Vec(vector_size=100, window=5, min_count=1, workers=4, sg=1, alpha=0.025, min_alpha=0.0001)
    model.build_vocab(train_data)
    model.train(train_data, total_examples=model.corpus_count, epochs=10, compute_loss=True, report_delay=1)
    model.save(model_path)
    print(f"Модель сохранена в {model_path}\nРазмер словаря: {len(model.wv.key_to_index)}")
    return model

train_data_sentences = [inner_list for item in train_data for inner_list in item['words']]
model = train_word2vec(train_data_sentences, "word2vec_v2.model")

train_data_sentences = [inner_list for item in train_data for inner_list in item['words']]

model = train_word2vec(train_data_sentences, model_path)

"""извлекает векторные представления слов из обученной модели Word2Vec.


"""

from typing import Union, List

def get_word_vectors(words, model, zero_vector_for_unknown=True):
    words = [words] if isinstance(words, str) else words
    vector_size = model.vector_size if hasattr(model, 'vector_size') else 100
    return [model.wv[word] if word in model.wv else np.zeros(vector_size) for word in words]

def cosine_distance(vec1: np.ndarray, vec2: np.ndarray) -> float:
    dot_product, norm1, norm2 = np.dot(vec1, vec2), np.linalg.norm(vec1), np.linalg.norm(vec2)
    return 1.0 if norm1 == 0 or norm2 == 0 else 1 - (dot_product / (norm1 * norm2))

# Анализ расстояний
def analyze_distances(word, model, test_words):
    if word not in model.wv:
        raise ValueError(f"Слово '{word}' отсутствует в модели")
    main_vector, distances = model.wv[word], {'similar': [], 'related': [], 'different': []}
    for group, words in test_words[word].items():
        for w in words:
            if w in model.wv:
                distances[group].append(cosine_distance(main_vector, model.wv[w]))
    return distances

metrics = {}
for word in test_words:
    distances = analyze_distances(word, model, test_words)
    metrics[word] = {
        'similar_mean': np.mean(distances['similar']),
        'related_mean': np.mean(distances['related']),
        'different_mean': np.mean(distances['different']),
        'diff_score': np.mean(distances['different']) - np.mean(distances['similar'])
    }

metrics

# Из лабы 1
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import WordNetLemmatizer

stemmer = SnowballStemmer(language='english')
lemmatizer = WordNetLemmatizer()

def split_into_sentences(text):
    # (?<!\w\.\w.) - проверяет, что нет слова, за которым следует точка и еще одно слово.
    # (?<![A-Z][a-z]\.) - проверяет, что перед текущей позицией нет заглавной буквы, за которой следует строчная буква и точка.
    # (?<=\.|\?|!) - проверяет, что перед текущей позицией находится точка, вопросительный знак или восклицательный знак.
    # \s - пробельный символ
    sentence_pattern = re.compile(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?|!)\s')

    # Находим все предложения в тексте
    sentences = sentence_pattern.split(text)

    # Удаляем пустые строки, если они есть
    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]

    return sentences

def tokenize_sentence(sentence):
    # Регулярное выражение для выделения отдельных токенов
    pattern = r"\+?\b[\w@.]+(?:'\w+)?\b|[:;,?.!]"
    return re.findall(pattern, sentence)

def find_emails(sentence):
    # [A-Za-z0-9._%+-]+ - часть почты до @
    # [A-Za-z0-9.-]+ - доменная часть - .com(или другого)
    # [A-Z|a-z]{2,} - доменный уровень (например, .com, .org)
    email_pattern = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b')

    # Находим все email-адреса в тексте
    emails = email_pattern.findall(sentence)

    return emails

def find_phone_number(sentence):
    # \+? - символ + есть ноль или 1 раз
    # [- (]? и [- )]? - разделитель в  виде -, () и пробел
    # \d{3} - три любых цифры
    #
    number_pattern = re.compile(r'\+?7?[- (]?\d{3}[- )]?\d{3}[- ]?\d{2}[- ]?\d{2}')

    # Находим все номера телефонов в тексте
    phone_numbers = number_pattern.findall(sentence)

    return phone_numbers

def find_dates(sentence):
    # Регулярное выражение для поиска дат
    date_pattern = re.compile(r'\b(\d{1,2})([./-]?)(\d{1,2})\2(\d{2,4})\b')

    # Находим все даты в тексте
    dates = date_pattern.findall(sentence)

    # Преобразуем найденные даты в строки
    formatted_dates = [f"{day}{separator}{month}{separator}{year}" for day, separator, month, year in dates]

    return formatted_dates

def tokenize_text(text):
    sentences = split_into_sentences(text)
    tokenized = []  # Список для хранения результатов по каждому предложению

    for sentence in sentences:
        sentence_data = {
            'tokens': [],  # Оригинальные токены
            'stems': [],   # Стемы
            'lemmas': [],  # Леммы
            'entities': []  # Специальные сущности (email, phone, date)
        }

        word_tokens = tokenize_sentence(sentence)

        for token in word_tokens:
            # Поиск специальных сущностей
            emails = find_emails(token)
            if emails:
                sentence_data['entities'].append({'type': 'email', 'value': emails[0]})
                continue

            phones = find_phone_number(token)
            if phones:
                sentence_data['entities'].append({'type': 'phone', 'value': phones[0]})
                continue

            dates = find_dates(token)
            if dates:
                sentence_data['entities'].append({'type': 'date', 'value': dates[0]})
                continue

            # Обычная обработка токена
            stem = stemmer.stem(token)
            lemma = lemmatizer.lemmatize(token)

            sentence_data['tokens'].append(token)
            sentence_data['stems'].append(stem)
            sentence_data['lemmas'].append(lemma)

        tokenized.append(sentence_data)  # Добавляем результат предложения в общий список

    return tokenized

text = 'This is an example of text. Second sentence.'

tokenize_text(text)

# Векторизация текста
def vectorize_text(text, model, tokenize=True):
    tokenized_sentences = tokenize_text(text) if tokenize else text
    vector_size = model.vector_size if hasattr(model, 'vector_size') else 100
    doc_token_vectors, doc_sentence_vectors = [], []

    for sentence_data in tokenized_sentences:
        tokens = sentence_data['tokens'] if tokenize else sentence_data
        token_vectors = get_word_vectors(tokens, model)
        if token_vectors:
            sentence_vector = np.mean(token_vectors, axis=0)
            doc_sentence_vectors.append(sentence_vector)
        doc_token_vectors.extend(token_vectors)

    return {
        'sentence_vectors': doc_sentence_vectors,
        'token_vectors': doc_token_vectors,
        'document_vector': np.mean(doc_sentence_vectors, axis=0) if doc_sentence_vectors else np.zeros(vector_size)
    }

vectorized_text = vectorize_text(train_data[0]['words'], model, tokenize=False)

type(train_data[0])

# Векторизация и сохранение датасета
def vectorize_and_save_dataset(model, train_data, test_data, output_dir="../assets/annotated-corpus-vectorized/"):
    def _process_subset(data, subset_name):
        for doc in tqdm(data, desc=f"Processing {subset_name} subset"):
            vectorization_result = vectorize_text(doc["words"], model, tokenize=False)
            class_dir = os.path.join(output_dir, subset_name, doc["class"])
            os.makedirs(class_dir, exist_ok=True)

            filename = os.path.join(class_dir, f"{doc['doc_id']}.tsv")
            with open(filename, "w", encoding="utf-8") as f:
                f.write(f"{doc['doc_id']}\t" + "\t".join(map("{:.6f}".format, vectorization_result["document_vector"])) + "\n")

    _process_subset(train_data, "train")
    _process_subset(test_data, "test")

vectorize_and_save_dataset(model, train_data, test_data)

