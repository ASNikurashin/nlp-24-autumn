{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alex4\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "\n",
    "# Загрузка ресурсов для лемматизатора\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_parquet(\"C:/Users/alex4/Desktop/code/data/test.parquet\")\n",
    "train_data = pd.read_parquet(\"C:/Users/alex4/Desktop/code/data/train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Email\n",
    "def match_emails(text):\n",
    "    pattern = r'\\b[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\\b'\n",
    "    return re.findall(pattern, text)\n",
    "\n",
    "# Телефоны\n",
    "def match_phones(text):\n",
    "    pattern = r'\\b(?:\\+7|8)[-\\s\\(]?\\d{3}[\\)\\s-]?\\d{3}[-]?\\d{2}[-]?\\d{2}\\b'\n",
    "    return re.findall(pattern, text)\n",
    "\n",
    "# Адреса\n",
    "def match_addresses(text):\n",
    "    pattern = r'г\\.\\s?[А-Яа-я\\-]+,?\\s+ул\\.?\\s?[А-Яа-я\\- ]+,?\\s+д\\.?\\s?\\d+,?\\s+кв\\.?\\s?\\d+'\n",
    "    return re.findall(pattern, text)\n",
    "\n",
    "# Эмотиконы\n",
    "def match_emoticons(text):\n",
    "    pattern = r'[:;=8][-~]?[)D\\(]'\n",
    "    return re.findall(pattern, text)\n",
    "\n",
    "# Формулы\n",
    "def match_math(text):\n",
    "    pattern = r'[\\w\\s]*=\\s*[\\w\\s\\*\\+\\-/\\(\\)\\^=]+'\n",
    "    return re.findall(pattern, text)\n",
    "\n",
    "# Сокращения\n",
    "def match_abbreviations(text):\n",
    "    pattern = r'\\b(?:[A-ZА-Я][a-zа-я]{0,3}\\.)\\s?[A-ZА-Я][a-zа-я]+\\b'\n",
    "    return re.findall(pattern, text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_all():\n",
    "    print(\"Emails:\", match_emails(\"Мой адрес: test.user@mail.ru и ещё один: abc@domain.com\"))\n",
    "    print(\"Phones:\", match_phones(\"Позвони мне +7-901-000-00-00 или 8(918)3213412\"))\n",
    "    print(\"Addresses:\", match_addresses(\"г. Санкт-Петербург, ул. Советская, д. 1294124, кв. 1\"))\n",
    "    print(\"Emoticons:\", match_emoticons(\"Привет :) как дела? Всё норм :D\"))\n",
    "    print(\"Math:\", match_math(\"Уравнение: a = b*c = (c+d)^2 и ещё x = y+z\"))\n",
    "    print(\"Abbreviations:\", match_abbreviations(\"Dr. Ivanov — известный специалист. Prof. Petrov тоже\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emails: ['test.user@mail.ru', 'abc@domain.com']\n",
      "Phones: ['8(918)3213412']\n",
      "Addresses: ['г. Санкт-Петербург, ул. Советская, д. 1294124, кв. 1']\n",
      "Emoticons: [':)', ':D']\n",
      "Math: [' a = b*c = (c+d)^2 и ещё x = y+z']\n",
      "Abbreviations: ['Dr. Ivanov', 'Prof. Petrov']\n"
     ]
    }
   ],
   "source": [
    "test_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def segment_and_tokenize_custom(text: str):\n",
    "    # Список сокращений, которые не должны разделяться\n",
    "    abbreviations = ['Dr.', 'Mr.', 'Ms.', 'Mrs.', 'Prof.', 'г.', 'ул.', 'д.', 'кв.', 'st.']\n",
    "\n",
    "    # Экранируем сокращения\n",
    "    for abbr in abbreviations:\n",
    "        text = text.replace(abbr, abbr.replace('.', '<DOT>'))\n",
    "\n",
    "    # Разделение на предложения по [.!?] + пробел + заглавная буква\n",
    "    sentence_split_pattern = re.compile(r'([.!?])\\s+(?=[А-ЯA-Z])')\n",
    "    text = sentence_split_pattern.sub(r'\\1<<<SENT>>>', text)\n",
    "\n",
    "    # Возвращаем точки обратно в сокращениях\n",
    "    text = text.replace('<DOT>', '.')\n",
    "\n",
    "    # Разделяем на предложения\n",
    "    sentences = text.split('<<<SENT>>>')\n",
    "\n",
    "    # Общий токенайзер\n",
    "    token_pattern = re.compile(r\"\"\"\n",
    "        \\b(?:\\+7|8)[-\\s(]?\\d{3}[-\\s)]?\\d{3}[-]?\\d{2}[-]?\\d{2}\\b |         # телефоны\n",
    "        \\b[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\\b |              # email\n",
    "        г\\.\\s?[А-Яа-я\\-]+,?\\s+ул\\.?\\s?[А-Яа-я\\- ]+,?\\s+д\\.?\\s?\\d+,?\\s+кв\\.?\\s?\\d+ |  # адреса\n",
    "        [:;=8][-~]?[)D\\(] |                                               # эмотиконы\n",
    "        (?:[A-ZА-Я][a-zа-я]{0,3}\\.)\\s?[A-ZА-Я][a-zа-я]+ |                 # сокращения + фамилия\n",
    "        \\b\\w+\\s*=\\s*[\\w\\s\\*\\+\\-/\\(\\)\\^=]+ |                               # формулы\n",
    "        [А-Яа-яA-Za-z0-9]+(?:-[А-Яа-яA-Za-z0-9]+)* |                     # слова\n",
    "        [.,!?;:] |                                                       # пунктуация\n",
    "        [^\\s]                                                            # прочее\n",
    "    \"\"\", re.VERBOSE)\n",
    "\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        tokens = token_pattern.findall(sentence.strip())\n",
    "        if tokens:\n",
    "            tokenized_sentences.append(tokens)\n",
    "\n",
    "    return tokenized_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предложение 1: ['Dr. Ivanov', 'отправил-письмо', 'на', 'адрес', 'abc@abc.abc', '.']\n",
      "Предложение 2: ['Позвони', 'по', 'номеру', '+', '7-901-000-00-00', '.']\n",
      "Предложение 3: ['Привет', ':)', ',', 'как', 'дела', '?']\n",
      "Предложение 4: ['Формула', ':', 'a = b*c = (c+d)^2']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    text = (\n",
    "        \"Dr. Ivanov отправил-письмо на адрес abc@abc.abc. \"\n",
    "        \"Позвони по номеру +7-901-000-00-00. \"\n",
    "        \"Привет:), как дела? \"\n",
    "        \"Формула: a = b*c = (c+d)^2\"\n",
    "    )\n",
    "\n",
    "    result = segment_and_tokenize_custom(text)\n",
    "    for i, sent in enumerate(result):\n",
    "        print(f\"Предложение {i+1}: {sent}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_tokens(tokens, language='english'):\n",
    "    \"\"\"\n",
    "    Выполняет стемминг списка токенов.\n",
    "    \"\"\"\n",
    "    stemmer = SnowballStemmer(language)\n",
    "    return [stemmer.stem(token) for token in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Выполняет лемматизацию списка токенов на английском языке.\n",
    "    (Для русского лучше использовать pymorphy2 — скажешь, добавим.)\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Токены:        ['refredgirator', 'generations']\n",
      "Стеммы:        ['refredgir', 'generat']\n",
      "Леммы:         ['refredgirator', 'generation']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    sentence = ['refredgirator','generations']\n",
    "\n",
    "    stems = stem_tokens(sentence)\n",
    "    lems = lemmatize_tokens(sentence)\n",
    "\n",
    "    print(\"Токены:       \", sentence)\n",
    "    print(\"Стеммы:       \", stems)\n",
    "    print(\"Леммы:        \", lems)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для формирования аннотации в формате .tsv\n",
    "def generate_tsv_annotation(text: str):\n",
    "    sentences = segment_and_tokenize_custom(text)\n",
    "    \n",
    "    result = []\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        # Токенизация, стемминг и лемматизация для каждого предложения\n",
    "        stems = stem_tokens(sentence, language='english')\n",
    "        lems = lemmatize_tokens(sentence)\n",
    "\n",
    "        for token, stem, lemma in zip(sentence, stems, lems):\n",
    "            result.append(f\"{token}\\t{stem}\\t{lemma}\")\n",
    "        \n",
    "        # Добавление пустой строки между предложениями\n",
    "        if idx < len(sentences) - 1:\n",
    "            result.append(\"\")\n",
    "    \n",
    "    # Сохранение в файл\n",
    "    with open(\"output_annotation.tsv\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(result))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tsv_annotation_from_df(df, output_path: str, language='english'):\n",
    "    result = []\n",
    "    В\n",
    "    for index, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        sentences = segment_and_tokenize_custom(text)\n",
    "        \n",
    "        for idx, sentence in enumerate(sentences):\n",
    "            if language == 'english':\n",
    "                stems = stem_tokens(sentence, language='english')\n",
    "                lems = lemmatize_tokens(sentence)\n",
    "            for token, stem, lemma in zip(sentence, stems, lems):\n",
    "                result.append(f\"{token}\\t{stem}\\t{lemma}\")\n",
    "            \n",
    "\n",
    "    # Сохранение в файл\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"\\n\".join(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример использования\n",
    "generate_tsv_annotation_from_df(train_data, \"train.tsv\", language=\"english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>Pakistan's Musharraf Says Won't Quit as Army C...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>Renteria signing a top-shelf deal Red Sox gene...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>Saban not going to Dolphins yet The Miami Dolp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>Today's NFL games PITTSBURGH at NY GIANTS Time...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>Nets get Carter from Raptors INDIANAPOLIS -- A...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label\n",
       "0       Wall St. Bears Claw Back Into the Black (Reute...      2\n",
       "1       Carlyle Looks Toward Commercial Aerospace (Reu...      2\n",
       "2       Oil and Economy Cloud Stocks' Outlook (Reuters...      2\n",
       "3       Iraq Halts Oil Exports from Main Southern Pipe...      2\n",
       "4       Oil prices soar to all-time record, posing new...      2\n",
       "...                                                   ...    ...\n",
       "119995  Pakistan's Musharraf Says Won't Quit as Army C...      0\n",
       "119996  Renteria signing a top-shelf deal Red Sox gene...      1\n",
       "119997  Saban not going to Dolphins yet The Miami Dolp...      1\n",
       "119998  Today's NFL games PITTSBURGH at NY GIANTS Time...      1\n",
       "119999  Nets get Carter from Raptors INDIANAPOLIS -- A...      1\n",
       "\n",
       "[120000 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tsv_annotation_from_df1(df, output_path: str, language='english'):\n",
    "    result = []\n",
    "\n",
    "    # Оставляем только строки с label == 0\n",
    "    filtered_df = df[df['label'] == 0]\n",
    "\n",
    "    for index, row in filtered_df.iterrows():\n",
    "        text = row['text']\n",
    "        sentences = segment_and_tokenize_custom(text)\n",
    "\n",
    "        for idx, sentence in enumerate(sentences):\n",
    "            if language == 'english':\n",
    "                stems = stem_tokens(sentence, language='english')\n",
    "                lems = lemmatize_tokens(sentence)\n",
    "\n",
    "            for token, stem, lemma in zip(sentence, stems, lems):\n",
    "                result.append(f\"{token}\\t{stem}\\t{lemma}\")\n",
    "\n",
    "    # Сохраняем результат в файл\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"\\n\".join(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_tsv_annotation_from_df1(test_data, \"output_label1.tsv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def generate_tsv_annotations_by_label(df, output_dir: str, language='english'):\n",
    "    \"\"\"\n",
    "    Генерирует TSV-файлы с аннотациями, распределяя их по папкам в соответствии с метками\n",
    "    \n",
    "    Параметры:\n",
    "        df: DataFrame с колонками 'text' и 'label'\n",
    "        output_dir: корневая директория для сохранения файлов\n",
    "        language: язык текста ('english' по умолчанию)\n",
    "    \"\"\"\n",
    "    # Создаем корневую директорию, если ее нет\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Группируем данные по меткам\n",
    "    grouped = df.groupby('label')\n",
    "    \n",
    "    for label, group in grouped:\n",
    "        # Создаем поддиректорию для метки\n",
    "        label_dir = os.path.join(output_dir, str(label))\n",
    "        Path(label_dir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Формируем имя файла\n",
    "        output_path = os.path.join(label_dir, f\"annotations_{label}.tsv\")\n",
    "        \n",
    "        result = []\n",
    "        \n",
    "        for index, row in group.iterrows():\n",
    "            text = row['text']\n",
    "            sentences = segment_and_tokenize_custom(text)\n",
    "            \n",
    "            for sentence in sentences:\n",
    "                if language == 'english':\n",
    "                    stems = stem_tokens(sentence, language='english')\n",
    "                    lems = lemmatize_tokens(sentence)\n",
    "                \n",
    "                for token, stem, lemma in zip(sentence, stems, lems):\n",
    "                    result.append(f\"{token}\\t{stem}\\t{lemma}\")\n",
    "\n",
    "                result.append(\"\")\n",
    "\n",
    "        # Сохранение в файл\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"\\n\".join(result))\n",
    "            \n",
    "        print(f\"Saved annotations for label '{label}' to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved annotations for label '0' to output_annotations\\0\\annotations_0.tsv\n",
      "Saved annotations for label '1' to output_annotations\\1\\annotations_1.tsv\n",
      "Saved annotations for label '2' to output_annotations\\2\\annotations_2.tsv\n",
      "Saved annotations for label '3' to output_annotations\\3\\annotations_3.tsv\n"
     ]
    }
   ],
   "source": [
    "# Предположим, df - ваш DataFrame с колонками 'text' и 'label'\n",
    "generate_tsv_annotations_by_label(train_data, \"output_annotations\", language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[test_data['label'] == 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Устанавливаем максимальное количество строк для вывода\n",
    "pd.set_option('display.max_rows', None)  # Устанавливает, чтобы показывались все строки\n",
    "\n",
    "# Выводим данные\n",
    "print(filtered_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
