# -*- coding: utf-8 -*-
"""lab1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AOVd5hTU7FZRIOtNL7bGQpTidLq9J0NX
"""

import numpy as np
import pandas as pd
import nltk
import os
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')

stemmer = SnowballStemmer(language='english')
lemmatizer = WordNetLemmatizer()

import re

train_data = pd.read_parquet("train.parquet")

test_data = pd.read_parquet("test.parquet")

def split_sen(text):
    sentence_pattern = re.compile(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?|!)\s')
    sentences = sentence_pattern.split(text)
    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]

    return sentences

def find_emails(sentence):
    email_pattern = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b')
    emails = email_pattern.findall(sentence)

    return emails

text = 'Please, send your feedback at this adress: {mail}'
emails = ["user.name@example.com","another-email@domain.org", "test123@sub.domain.co.uk"]
for em in emails:
    print(find_emails(text.format(mail=em)))

def find_phone_number(sentence):
    number_pattern = re.compile(r'\+?7?[- (]?\d{3}[- )]?\d{3}[- ]?\d{2}[- ]?\d{2}')
    phone_numbers = number_pattern.findall(sentence)

    return phone_numbers

text_phone = 'Please, call us back: {number}'
phones = ["+79261234567","89261234567", "79261234567","+7 926 123 45 67", "8(926)123-45-67"]
for phone in phones:
    print(find_phone_number(text_phone.format(number=phone)))

def find_dates(sentence):
    # Регулярное выражение для поиска дат
    date_pattern = re.compile(r'\b(\d{1,2})([./-]?)(\d{1,2})\2(\d{2,4})\b')
    dates = date_pattern.findall(sentence)
    formatted_dates = [f"{day}{separator}{month}{separator}{year}" for day, separator, month, year in dates]

    return formatted_dates

text_date = 'Date to meet: {dat}'
dates = ['19.06.2024', '19-06-2024', '10/12/24']
for date in dates:
    print(find_dates(text_date.format(dat=date)))

def tokenize_sen(sentence):
    # Регулярное выражение для выделения отдельных токенов
    pattern = r"\+?\b[\w@.]+(?:'\w+)?\b|[:;,?.!]"
    return re.findall(pattern, sentence)

tokens = tokenize_sen(text_phones)

print(text_phones)

print(tokens)

def tokenizer(text):
    sentences = split_sen(text)
    tokenized = []  # Список для хранения результатов по каждому предложению

    for sentence in sentences:
        sentence_data = {
            'tokens': [],  # Оригинальные токены
            'stems': [],   # Стемы
            'lemmas': [],  # Леммы
            'entities': []  # Специальные сущности (email, phone, date)
        }
        word_tokens = tokenize_sen(sentence)

        for token in word_tokens:
            emails = find_emails(token)
            if emails:
                sentence_data['entities'].append({'type': 'email', 'value': emails[0]})
                continue
            phones = find_phone_number(token)
            if phones:
                sentence_data['entities'].append({'type': 'phone', 'value': phones[0]})
                continue
            dates = find_dates(token)
            if dates:
                sentence_data['entities'].append({'type': 'date', 'value': dates[0]})
                continue

            stem = stemmer.stem(token)
            lemma = lemmatizer.lemmatize(token)
            sentence_data['tokens'].append(token)
            sentence_data['stems'].append(stem)
            sentence_data['lemmas'].append(lemma)

        tokenized.append(sentence_data)

    return tokenized

def format_to_tsv(tokenized_data):
    tsv_lines = []

    for sentence_data in tokenized_data:
        for token, stem, lemma in zip(sentence_data['tokens'], sentence_data['stems'], sentence_data['lemmas']):
            tsv_lines.append(f"{token}\t{stem}\t{lemma}")
        for entity in sentence_data['entities']:
            tsv_lines.append(f"{entity['type']}\t{entity['value']}")
        tsv_lines.append('')
    return '\n'.join(tsv_lines[:-1])

tokenized_data = tokenizer(train_data['text'][55])

all_tokens = [token for sentence_data in tokenized_data for token in sentence_data['tokens']]

tsv_output = format_to_tsv(tokenized_data)
print(tsv_output)

print(tokenizer(train_data['text'][5]))

def generate_tsv(train_data, test_data):
    path_to_save = ''
    classes = sorted(train_data['label'].unique())
    for type, data in [('train', train_data), ('test',test_data)]:
        for cl in classes:
            folder_name = path_to_save + type + '/' + str(cl)
            annotated_file = ''
            if not os.path.exists(folder_name):
                os.makedirs(folder_name)

            for text_item in data.loc[data['label'] == cl]['text'].to_list():
                annotated_file += tokenizer(text_item)

            file_path = folder_name + '/' + str(cl) + '.tsv'
            with open(file_path, 'w', encoding='utf-8') as tsv_file:
                tsv_file.write(annotated_file)

generate_tsv(train_data, test_data)

